import os
import asyncio
import feedparser
import json
import requests
import time
import google.generativeai as genai
from datetime import datetime, timedelta, timezone
from dotenv import load_dotenv
from telegram import Bot
from supabase import create_client, Client

load_dotenv()

# –ö–ª—é—á–∏
GEMINI_API_KEY = os.getenv("GEMINI_API_KEY")
TELEGRAM_BOT_TOKEN = os.getenv("TELEGRAM_BOT_TOKEN")
TELEGRAM_CHAT_ID = os.getenv("TELEGRAM_CHAT_ID")
SUPABASE_URL = os.getenv("SUPABASE_URL")
SUPABASE_KEY = os.getenv("SUPABASE_KEY")

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ AI
genai.configure(api_key=GEMINI_API_KEY)
model = genai.GenerativeModel('gemini-flash-latest')

# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è Supabase
supabase: Client = None
if SUPABASE_URL and SUPABASE_KEY:
    supabase = create_client(SUPABASE_URL, SUPABASE_KEY)

def fetch_hacker_news_cases():
    print("üîç –ò—â—É –Ω–∞ Hacker News...")
    timestamp = int((datetime.now(timezone.utc) - timedelta(days=1)).timestamp())
    query = "AI revenue OR AI profit OR AI SaaS OR AI MRR"
    url = f"https://hn.algolia.com/api/v1/search?query={query}&tags=story&numericFilters=created_at_i>{timestamp}"
    found = []
    try:
        r = requests.get(url, timeout=10)
        if r.status_code == 200:
            hits = r.json().get('hits', [])
            for hit in hits:
                found.append({
                    'title': hit['title'],
                    'text': hit.get('story_text', '')[:2000],
                    'url': hit.get('url') or f"https://news.ycombinator.com/item?id={hit['objectID']}",
                    'source': 'Hacker News'
                })
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ Hacker News: {e}")
    return found

def fetch_github_trending():
    print("üîç –ò—â—É —Ç—Ä–µ–Ω–¥—ã –Ω–∞ GitHub (AI)...")
    date_str = (datetime.now(timezone.utc) - timedelta(days=2)).strftime('%Y-%m-%d')
    url = f"https://api.github.com/search/repositories?q=topic:ai+created:>{date_str}&sort=stars&order=desc"
    found = []
    try:
        r = requests.get(url, timeout=10)
        if r.status_code == 200:
            items = r.json().get('items', [])[:5]
            for item in items:
                found.append({
                    'title': f"GitHub Trend: {item['name']}",
                    'text': item['description'] or 'No description',
                    'url': item['html_url'],
                    'source': 'GitHub'
                })
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ GitHub: {e}")
    return found

def fetch_reddit_cases():
    print("üîç –ò—â—É –Ω–∞ Reddit (—á–µ—Ä–µ–∑ JSON)...")
    subreddits = ["SideProject", "SaaS", "Entrepreneur", "AiMoneyMaking", "IndieHackers", "solopreneur"]
    search_queries = ["AI revenue", "AI MRR", "AI profit", "AI case study"]
    found_posts = []
    
    headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'}
    limit_date = datetime.now(timezone.utc) - timedelta(days=1)

    for sub_name in subreddits:
        try:
            # –°–Ω–∞—á–∞–ª–∞ –ø—Ä–æ–±—É–µ–º –ø—Ä–æ—Å—Ç–æ–π –ª–∏—Å—Ç–∏–Ω–≥ —Å–∞–±—Ä–µ–¥–¥–∏—Ç–∞, –µ—Å–ª–∏ —ç—Ç–æ —Å–ø–µ—Ü–∏—Ñ–∏—á–Ω—ã–π —Å–∞–±
            if sub_name == "AiMoneyMaking":
                 url = f"https://www.reddit.com/r/{sub_name}/new.json?limit=10"
                 r = requests.get(url, headers=headers, timeout=10)
                 if r.status_code == 200:
                    posts = r.json().get('data', {}).get('children', [])
                    for post in posts:
                        p_data = post['data']
                        created_utc = datetime.fromtimestamp(p_data['created_utc'], timezone.utc)
                        if created_utc > limit_date:
                            # –î–µ—Ç–∞–ª–∏ –ø–æ—Å—Ç–∞
                            post_url = f"https://www.reddit.com{p_data['permalink']}"
                            # –ò–Ω–æ–≥–¥–∞ —Ç–µ–∫—Å—Ç —É–∂–µ –µ—Å—Ç—å –≤ –ª–∏—Å—Ç–∏–Ω–≥–µ
                            text = p_data.get('selftext', '')
                            # –ï—Å–ª–∏ —Ç–µ–∫—Å—Ç–∞ –Ω–µ—Ç, –º–æ–∂–Ω–æ –ø–æ–ø—Ä–æ–±–æ–≤–∞—Ç—å –∑–∞–π—Ç–∏ –≤–Ω—É—Ç—Ä—å (–¥–æ–ø. –∑–∞–ø—Ä–æ—Å), –Ω–æ –ø–æ–∫–∞ –æ–≥—Ä–∞–Ω–∏—á–∏–º—Å—è –ª–∏—Å—Ç–∏–Ω–≥–æ–º –¥–ª—è —Å–∫–æ—Ä–æ—Å—Ç–∏
                            
                            found_posts.append({
                                'title': p_data['title'],
                                'text': text[:2000],
                                'url': post_url,
                                'source': f"Reddit (r/{sub_name})"
                            })
            
            # –ü–æ–∏—Å–∫ –ø–æ –∫–ª—é—á–µ–≤—ã–º —Å–ª–æ–≤–∞–º –¥–ª—è –æ—Å—Ç–∞–ª—å–Ω—ã—Ö
            else:
                for query in search_queries:
                    url = f"https://www.reddit.com/r/{sub_name}/search.json?q={query}&sort=new&restrict_sr=1&limit=5"
                    r = requests.get(url, headers=headers, timeout=10)
                    if r.status_code == 200:
                        posts = r.json().get('data', {}).get('children', [])
                        for post in posts:
                            p_data = post['data']
                            created_utc = datetime.fromtimestamp(p_data['created_utc'], timezone.utc)
                            if created_utc > limit_date:
                                found_posts.append({
                                    'title': p_data['title'],
                                    'text': p_data.get('selftext', '')[:2000],
                                    'url': f"https://www.reddit.com{p_data['permalink']}",
                                    'source': f"Reddit (r/{sub_name})"
                                })
                    time.sleep(1) # –í–µ–∂–ª–∏–≤–æ—Å—Ç—å –∫ API
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ Reddit r/{sub_name}: {e}")
            
    # –£–¥–∞–ª—è–µ–º –¥—É–±–ª–∏–∫–∞—Ç—ã
    unique_posts = {p['url']: p for p in found_posts}.values()
    return list(unique_posts)

def fetch_rss_cases():
    print("üîç –ò—â—É –≤ RSS –ª–µ–Ω—Ç–∞—Ö...")
    RSS_FEEDS = ["https://medium.com/feed/tag/ai-monetization", "https://www.indiehackers.com/rss"]
    news_items = []
    yesterday = datetime.now(timezone.utc) - timedelta(days=1)
    for url in RSS_FEEDS:
        try:
            feed = feedparser.parse(url)
            for entry in feed.entries:
                published_parsed = getattr(entry, 'published_parsed', None)
                if published_parsed:
                    pub_date = datetime(*published_parsed[:6], tzinfo=timezone.utc)
                    if pub_date > yesterday:
                        news_items.append({
                            'title': entry.title,
                            'text': entry.summary if 'summary' in entry else '',
                            'url': entry.link,
                            'source': 'RSS'
                        })
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ RSS {url}: {e}")
    return news_items

def analyze_cases(cases):
    if not cases: return None, None

    context = ""
    for i, c in enumerate(cases[:15], 1):
        context += f"--- SOURCE {i} ({c['source']}) ---\nTitle: {c['title']}\nContent: {c['text']}\nURL: {c['url']}\n\n"

    prompt = f"""
    –¢—ã ‚Äî –≤–µ–¥—É—â–∏–π –∞–Ω–∞–ª–∏—Ç–∏–∫ –≤–µ–Ω—á—É—Ä–Ω–æ–≥–æ —Ñ–æ–Ω–¥–∞, —Å–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä—É—é—â–∏–π—Å—è –Ω–∞ AI-—Å—Ç–∞—Ä—Ç–∞–ø–∞—Ö –∏ –º–∏–∫—Ä–æ-SaaS. 
    –¢–≤–æ—è –∑–∞–¥–∞—á–∞: –Ω–∞ –æ—Å–Ω–æ–≤–µ –≤—Ö–æ–¥—è—â–∏—Ö –¥–∞–Ω–Ω—ã—Ö —Å–æ—Å—Ç–∞–≤–∏—Ç—å –æ—Ç—á–µ—Ç –æ 2-3 —Å–∞–º—ã—Ö –ø–µ—Ä—Å–ø–µ–∫—Ç–∏–≤–Ω—ã—Ö –∏ —Ä–µ–∞–ª—å–Ω—ã—Ö –∫–µ–π—Å–∞—Ö –∑–∞—Ä–∞–±–æ—Ç–∫–∞.

    –í–´–î–ê–ô –û–¢–í–ï–¢ –°–¢–†–û–ì–û –í –§–û–†–ú–ê–¢–ï JSON. 
    –§–û–†–ú–ê–¢ JSON:
    {{
      "telegram_post": "–¢–µ–∫—Å—Ç –æ–±—â–µ–≥–æ –ø–æ—Å—Ç–∞ –¥–ª—è –∫–∞–Ω–∞–ª–∞...",
      "cases": [
        {{
          "title": "–ù–∞–∑–≤–∞–Ω–∏–µ –∫–µ–π—Å–∞",
          "profit": "–ü—Ä–æ—Ñ–∏—Ç (—Ü–∏—Ñ—Ä—ã/–æ–ø–∏—Å–∞–Ω–∏–µ)",
          "profit_num": 1234.5,
          "category": "–ö–∞—Ç–µ–≥–æ—Ä–∏—è",
          "tags": ["Tag1", "Tag2"],
          "difficulty_score": 5,
          "scheme": "–ü–æ—à–∞–≥–æ–≤–∞—è —Ä–µ–∞–ª–∏–∑–∞—Ü–∏—è",
          "stack": "–¢–µ—Ö–Ω–æ–ª–æ–≥–∏—á–µ—Å–∫–∏–π —Å—Ç–µ–∫",
          "url": "–°—Å—ã–ª–∫–∞ –Ω–∞ –∏—Å—Ç–æ—á–Ω–∏–∫",
          "source": "–ò—Å—Ç–æ—á–Ω–∏–∫"
        }}
      ]
    }}

    –ü–†–ê–í–ò–õ–ê –¢–ï–õ–ï–ì–†–ê–ú-–ü–û–°–¢–ê (HTML):
    - –ó–∞–≥–æ–ª–æ–≤–æ–∫: üî• <b>–ö–ï–ô–°–´ –ó–ê–†–ê–ë–û–¢–ö–ê: AI –ú–û–ù–ï–¢–ò–ó–ê–¶–ò–Ø</b>
    - –í–≤–æ–¥–Ω–∞—è —á–∞—Å—Ç—å: –∫–æ—Ä–æ—Ç–∫–∏–π, –¥–µ—Ä–∑–∫–∏–π –∏–Ω—Å–∞–π—Ç –æ —Ç–µ–∫—É—â–µ–º —Ä—ã–Ω–∫–µ AI (1-2 –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è).
    - –ö–∞–∂–¥—ã–π –∫–µ–π—Å –æ—Ñ–æ—Ä–º–∏ –ø–æ —à–∞–±–ª–æ–Ω—É:
      üöÄ <b>–ö–µ–π—Å: [–ù–∞–∑–≤–∞–Ω–∏–µ]</b>
      üí∞ –ü—Ä–æ—Ñ–∏—Ç: <i>[–û–ø–∏—Å–∞–Ω–∏–µ –ø—Ä–æ—Ñ–∏—Ç–∞]</i>
      üõ† –°—Ç–µ–∫: <code>[–ò–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç—ã]</code>
      üìç <a href="[url]">–ß–∏—Ç–∞—Ç—å –ø–æ–¥—Ä–æ–±–Ω–µ–µ –≤ –∏—Å—Ç–æ—á–Ω–∏–∫–µ</a>

    - –í –∫–æ–Ω—Ü–µ –¥–æ–±–∞–≤—å —Ä–∞–∑–¥–µ–ª–∏—Ç–µ–ª—å –∏ —Ç–µ–≥–∏:
      _______________________
      #AI #MoneyCases #SaaS #[Category]

    - –ò—Å–ø–æ–ª—å–∑—É–π –¢–û–õ–¨–ö–û <b>, <i>, <a>, <code>.
    - –î–ª—è –Ω–æ–≤—ã—Ö —Å—Ç—Ä–æ–∫ –∏—Å–ø–æ–ª—å–∑—É–π \n.
    """

    response = model.generate_content(prompt)
    try:
        text = response.text.strip()
        if '```json' in text:
            text = text.split('```json')[1].split('```')[0]
        elif '```' in text:
            text = text.split('```')[1].split('```')[0]
        
        result = json.loads(text.strip())
        post = result.get("telegram_post", "")
        post = post.replace("<br>", "\n").replace("<br/>", "\n").replace("<p>", "").replace("</p>", "\n")
        return post, result.get("cases")
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ JSON: {e}")
        return None, None

def save_to_supabase(cases):
    if not supabase or not cases: return
    for case in cases:
        try:
            supabase.table("ai_money_cases").upsert({
                "title": case['title'], "profit": case['profit'], "profit_num": case.get('profit_num', 0),
                "category": case.get('category', 'Other'), "tags": case.get('tags', []),
                "difficulty_score": case.get('difficulty_score', 5), "scheme": case['scheme'],
                "stack": case['stack'], "url": case['url'], "source": case['source'],
                "created_at": datetime.now(timezone.utc).isoformat()
            }, on_conflict="url").execute()
            print(f"üíæ –°–æ—Ö—Ä–∞–Ω–µ–Ω–æ: {case['title']}")
        except Exception as e:
            print(f"‚ùå Supabase error: {e}")

async def send_to_telegram(text):
    if not TELEGRAM_BOT_TOKEN or not TELEGRAM_CHAT_ID or not text: return
    bot = Bot(token=TELEGRAM_BOT_TOKEN)
    async with bot:
        await bot.send_message(chat_id=TELEGRAM_CHAT_ID, text=text, parse_mode='HTML', disable_web_page_preview=True)

async def main():
    print("üöÄ –ì–õ–û–ë–ê–õ–¨–ù–´–ô –ü–û–ò–°–ö –ö–ï–ô–°–û–í...")
    hn = fetch_hacker_news_cases()
    gh = fetch_github_trending()
    rd = fetch_reddit_cases()
    rs = fetch_rss_cases()
    
    all_cases = hn + gh + rd + rs
    print(f"üìä –ù–∞–π–¥–µ–Ω–æ –º–∞—Ç–µ—Ä–∏–∞–ª–æ–≤: {len(all_cases)}")
    
    if all_cases:
        report, cases_list = analyze_cases(all_cases)
        if report:
            print(report)
            await send_to_telegram(report)
            if cases_list: save_to_supabase(cases_list)
    else:
        print("‚ùå –ù–∏—á–µ–≥–æ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ.")

if __name__ == "__main__":
    asyncio.run(main())